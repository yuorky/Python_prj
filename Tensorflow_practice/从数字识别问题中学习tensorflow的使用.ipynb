{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y63qiu\\AppData\\Local\\Continuum\\anaconda3\\envs\\yorkml\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-cf764b4917b0>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\y63qiu\\AppData\\Local\\Continuum\\anaconda3\\envs\\yorkml\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\y63qiu\\AppData\\Local\\Continuum\\anaconda3\\envs\\yorkml\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\y63qiu\\AppData\\Local\\Continuum\\anaconda3\\envs\\yorkml\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting C:\\Users\\y63qiu\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\y63qiu\\AppData\\Local\\Continuum\\anaconda3\\envs\\yorkml\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting C:\\Users\\y63qiu\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\y63qiu\\AppData\\Local\\Continuum\\anaconda3\\envs\\yorkml\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting C:\\Users\\y63qiu\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting C:\\Users\\y63qiu\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\y63qiu\\AppData\\Local\\Continuum\\anaconda3\\envs\\yorkml\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"C:\\\\Users\\\\y63qiu\\\\\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 3 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "maxarg = tf.argmax(mnist.train.labels,1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(maxarg))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 55000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data size:\", mnist.train.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating data size: 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"Validating data size:\", mnist.validation.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data size: 10000\n"
     ]
    }
   ],
   "source": [
    "print('Testing data size:', mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import所需以及设置神经网络基本参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 配置神经网络相关的参数\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "LAYER1_NODE = 500\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "\n",
    "REGU_RATE = 0.0001\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义一些辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数用于计算神经网络的向前传播\n",
    "\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 如果没有提供滑动平均类时，直接使用参数当前取值\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        \n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    \n",
    "    else:\n",
    "        # 计算出变量的滑动平均值，再计算向前传播的结果\n",
    "        layer1 = tf.nn.relu(\n",
    "            tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        \n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数用于定义神经网络训练的过程\n",
    "\n",
    "def train(minst):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    # 定义各层的 weight 和 biases\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2) # 这里的y既是神经网络的输出，输出之后会通过例如sigmoid之类的处理\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False) # 训练的轮数，为不可训练的变量\n",
    "    \n",
    "    # 滑动平均相关\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    \n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 用交叉熵作为损失函数， 直接使用sparse_softmax_cross_entropy_with_logits来进行 softmax回归处理 和 交叉熵的计算\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_,1))\n",
    "    \n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 优化目标中加入L2正则化\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGU_RATE)\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    Loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    # 设置衰减指数学习率\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples/BATCH_SIZE, LEARNING_RATE_DECAY)\n",
    "    \n",
    "    # 使用 Gradient Descent 算法来优化损失函数\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(Loss, global_step=global_step)\n",
    "    \n",
    "    # 训练滑动平均\n",
    "    with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "        \n",
    "    # 验证滑动平均模型向前传播是否正确\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y,1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 开始会话， 开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # 准备验证数据， 用于判断神经网络训练过程的停止\n",
    "        validate_feed = {x:mnist.validation.images, y_:mnist.validation.labels}\n",
    "        \n",
    "        # 准备测试数据做模型的评价\n",
    "        test_feed = {x:mnist.test.images, y_:mnist.test.labels}\n",
    "        \n",
    "        # 迭代训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            #每1000轮输出一次在验证集上的结果\n",
    "            if i % 1000 ==0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g\" %(i, validate_acc))\n",
    "                \n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x:xs, y_:ys})\n",
    "            \n",
    "        # 在训练结束之后，用测试集验证模型准确率\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"After %d training step(s), test accuracy using average model is %g\"%(TRAINING_STEPS, test_acc))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"C:\\\\Users\\\\y63qiu\\\\\", one_hot=True)\n",
    "    train(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\y63qiu\\train-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\y63qiu\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:\\Users\\y63qiu\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\y63qiu\\t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step(s), validation accuracy using average model is 0.1386\n",
      "After 1000 training step(s), validation accuracy using average model is 0.9774\n",
      "After 2000 training step(s), validation accuracy using average model is 0.9816\n",
      "After 3000 training step(s), validation accuracy using average model is 0.9828\n",
      "After 4000 training step(s), validation accuracy using average model is 0.9834\n",
      "After 5000 training step(s), validation accuracy using average model is 0.984\n",
      "After 6000 training step(s), validation accuracy using average model is 0.984\n",
      "After 7000 training step(s), validation accuracy using average model is 0.9848\n",
      "After 8000 training step(s), validation accuracy using average model is 0.9844\n",
      "After 9000 training step(s), validation accuracy using average model is 0.985\n",
      "After 10000 training step(s), validation accuracy using average model is 0.9836\n",
      "After 11000 training step(s), validation accuracy using average model is 0.9842\n",
      "After 12000 training step(s), validation accuracy using average model is 0.9844\n",
      "After 13000 training step(s), validation accuracy using average model is 0.9846\n",
      "After 14000 training step(s), validation accuracy using average model is 0.9838\n",
      "After 15000 training step(s), validation accuracy using average model is 0.9838\n",
      "After 16000 training step(s), validation accuracy using average model is 0.985\n",
      "After 17000 training step(s), validation accuracy using average model is 0.984\n",
      "After 18000 training step(s), validation accuracy using average model is 0.9848\n",
      "After 19000 training step(s), validation accuracy using average model is 0.9838\n",
      "After 20000 training step(s), validation accuracy using average model is 0.984\n",
      "After 21000 training step(s), validation accuracy using average model is 0.984\n",
      "After 22000 training step(s), validation accuracy using average model is 0.9842\n",
      "After 23000 training step(s), validation accuracy using average model is 0.9844\n",
      "After 24000 training step(s), validation accuracy using average model is 0.9846\n",
      "After 25000 training step(s), validation accuracy using average model is 0.9838\n",
      "After 26000 training step(s), validation accuracy using average model is 0.985\n",
      "After 27000 training step(s), validation accuracy using average model is 0.9844\n",
      "After 28000 training step(s), validation accuracy using average model is 0.9842\n",
      "After 29000 training step(s), validation accuracy using average model is 0.9846\n",
      "After 30000 training step(s), test accuracy using average model is 0.9833\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y63qiu\\AppData\\Local\\Continuum\\anaconda3\\envs\\yorkml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    " if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
