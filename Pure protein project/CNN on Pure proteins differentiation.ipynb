{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import SubsetRandomSampler as SRS \n",
    "from torch.utils.data import DataLoader\n",
    "from data.Dataset import Proteins\n",
    "from models import resnet101\n",
    "from sklearn.model_selection import train_test_split\n",
    "from config import DefaultConfig\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = DefaultConfig()\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
    "])\n",
    "\n",
    "all_dataset = Proteins(root=opt.root, transforms=transform)\n",
    "\n",
    "# split training and test set 7:3\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(all_dataset.labels)),\n",
    "    test_size=0.3,\n",
    "    stratify=all_dataset.labels\n",
    ")\n",
    "\n",
    "train_sampler = SRS(train_idx)\n",
    "test_sampler = SRS(test_idx)\n",
    "train_loader = DataLoader(all_dataset, batch_size=10, sampler=train_sampler)\n",
    "test_loader = DataLoader(all_dataset, batch_size=10, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.sampler.indices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.sampler.indices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, train_loader=train_loader):\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):\n",
    "                # max-norm constraint\n",
    "                #for name, param in model.named_parameters():\n",
    "                #    if 'bias' not in name:\n",
    "                #        norm = param.norm(2,dim=0,keepdim=True)\n",
    "                #        desired = torch.clamp(norm,0,3)\n",
    "                #        param = param*(desired/(1e-8+norm))\n",
    "                        \n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)  # 1 is the dimension\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        # Decay learning rate\n",
    "\n",
    "        epoch_loss = running_loss / train_loader.sampler.indices.shape[0]\n",
    "        epoch_acc = running_corrects.item() / train_loader.sampler.indices.shape[0]\n",
    "\n",
    "        print('{} Loss: {: .4f} Acc: {: .4f}'.format(\n",
    "            'Train', epoch_loss, epoch_acc\n",
    "        ))\n",
    "        scheduler.step()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60\n",
    "    ))\n",
    "\n",
    "    # load best model weights\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader=test_loader):\n",
    "    test_corrects = 0\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)  # 1 is the dimension\n",
    "\n",
    "        test_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    test_acc = test_corrects.double() / test_loader.sampler.indices.shape[0]\n",
    "\n",
    "    return test_corrects, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029\n",
      "----------\n",
      "Train Loss:  2.5297 Acc:  0.5275\n",
      "Epoch 129\n",
      "----------\n",
      "Train Loss:  1.6457 Acc:  0.4908\n",
      "Epoch 229\n",
      "----------\n",
      "Train Loss:  0.8686 Acc:  0.4908\n",
      "Epoch 329\n",
      "----------\n",
      "Train Loss:  0.7080 Acc:  0.5505\n",
      "Epoch 429\n",
      "----------\n",
      "Train Loss:  0.7082 Acc:  0.5550\n",
      "Epoch 529\n",
      "----------\n",
      "Train Loss:  0.7113 Acc:  0.5321\n",
      "Epoch 629\n",
      "----------\n",
      "Train Loss:  0.6819 Acc:  0.5321\n",
      "Epoch 729\n",
      "----------\n",
      "Train Loss:  0.6791 Acc:  0.5826\n",
      "Epoch 829\n",
      "----------\n",
      "Train Loss:  0.6937 Acc:  0.5642\n",
      "Epoch 929\n",
      "----------\n",
      "Train Loss:  0.6850 Acc:  0.5688\n",
      "Epoch 1029\n",
      "----------\n",
      "Train Loss:  0.6745 Acc:  0.5734\n",
      "Epoch 1129\n",
      "----------\n",
      "Train Loss:  0.6722 Acc:  0.5505\n",
      "Epoch 1229\n",
      "----------\n",
      "Train Loss:  0.6649 Acc:  0.5459\n",
      "Epoch 1329\n",
      "----------\n",
      "Train Loss:  0.6721 Acc:  0.5596\n",
      "Epoch 1429\n",
      "----------\n",
      "Train Loss:  0.6625 Acc:  0.5917\n",
      "Epoch 1529\n",
      "----------\n",
      "Train Loss:  0.6751 Acc:  0.5505\n",
      "Epoch 1629\n",
      "----------\n",
      "Train Loss:  0.6645 Acc:  0.5780\n",
      "Epoch 1729\n",
      "----------\n",
      "Train Loss:  0.6674 Acc:  0.5642\n",
      "Epoch 1829\n",
      "----------\n",
      "Train Loss:  0.6590 Acc:  0.6239\n",
      "Epoch 1929\n",
      "----------\n",
      "Train Loss:  0.6644 Acc:  0.5505\n",
      "Epoch 2029\n",
      "----------\n",
      "Train Loss:  0.6530 Acc:  0.6376\n",
      "Epoch 2129\n",
      "----------\n",
      "Train Loss:  0.6565 Acc:  0.6193\n",
      "Epoch 2229\n",
      "----------\n",
      "Train Loss:  0.6679 Acc:  0.5688\n",
      "Epoch 2329\n",
      "----------\n",
      "Train Loss:  0.6580 Acc:  0.5734\n",
      "Epoch 2429\n",
      "----------\n",
      "Train Loss:  0.6563 Acc:  0.5917\n",
      "Epoch 2529\n",
      "----------\n",
      "Train Loss:  0.6588 Acc:  0.6147\n",
      "Epoch 2629\n",
      "----------\n",
      "Train Loss:  0.6696 Acc:  0.5642\n",
      "Epoch 2729\n",
      "----------\n",
      "Train Loss:  0.6564 Acc:  0.5963\n",
      "Epoch 2829\n",
      "----------\n",
      "Train Loss:  0.6588 Acc:  0.5734\n",
      "Epoch 2929\n",
      "----------\n",
      "Train Loss:  0.6509 Acc:  0.6055\n",
      "Training complete in 6m 35s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Linear(num_in_ft, 2)\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.9)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer_AB40_SYN,\n",
    "                                step_size=1, gamma=0.8)\n",
    "\n",
    "scheduler_2 = ReduceLROnPlateau(optimizer_AB40_SYN, mode='max', factor=0.5,\n",
    "                                patience=1, verbose=True)\n",
    "\n",
    "resnet101_AB40_SYN = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN, scheduler,\n",
    "                                 num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(scheduler.last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2379400392853824e-05"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029\n",
      "----------\n",
      "Train Loss:  1.5226 Acc:  0.4771\n",
      "Epoch 129\n",
      "----------\n",
      "Train Loss:  1.0241 Acc:  0.5000\n",
      "Epoch 229\n",
      "----------\n",
      "Train Loss:  0.9459 Acc:  0.5596\n",
      "Epoch 329\n",
      "----------\n",
      "Train Loss:  0.7949 Acc:  0.5459\n",
      "Epoch 429\n",
      "----------\n",
      "Train Loss:  0.7848 Acc:  0.5183\n",
      "Epoch 529\n",
      "----------\n",
      "Train Loss:  0.7076 Acc:  0.6009\n",
      "Epoch 629\n",
      "----------\n",
      "Train Loss:  0.7236 Acc:  0.5229\n",
      "Epoch 729\n",
      "----------\n",
      "Train Loss:  0.7045 Acc:  0.4954\n",
      "Epoch 829\n",
      "----------\n",
      "Train Loss:  0.6270 Acc:  0.6193\n",
      "Epoch 929\n",
      "----------\n",
      "Train Loss:  0.7204 Acc:  0.6055\n",
      "Epoch 1029\n",
      "----------\n",
      "Train Loss:  0.6324 Acc:  0.6376\n",
      "Epoch 1129\n",
      "----------\n",
      "Train Loss:  0.6475 Acc:  0.5734\n",
      "Epoch 1229\n",
      "----------\n",
      "Train Loss:  0.5841 Acc:  0.6697\n",
      "Epoch 1329\n",
      "----------\n",
      "Train Loss:  0.5977 Acc:  0.6972\n",
      "Epoch 1429\n",
      "----------\n",
      "Train Loss:  0.4942 Acc:  0.7615\n",
      "Epoch 1529\n",
      "----------\n",
      "Train Loss:  0.4883 Acc:  0.7890\n",
      "Epoch 1629\n",
      "----------\n",
      "Train Loss:  0.5273 Acc:  0.7523\n",
      "Epoch 1729\n",
      "----------\n",
      "Train Loss:  0.3617 Acc:  0.8853\n",
      "Epoch 1829\n",
      "----------\n",
      "Train Loss:  0.4731 Acc:  0.7661\n",
      "Epoch 1929\n",
      "----------\n",
      "Train Loss:  0.5234 Acc:  0.7661\n",
      "Epoch 2029\n",
      "----------\n",
      "Train Loss:  0.5978 Acc:  0.7018\n",
      "Epoch 2129\n",
      "----------\n",
      "Train Loss:  0.5588 Acc:  0.7156\n",
      "Epoch 2229\n",
      "----------\n",
      "Train Loss:  0.4074 Acc:  0.8211\n",
      "Epoch 2329\n",
      "----------\n",
      "Train Loss:  0.3262 Acc:  0.8945\n",
      "Epoch 2429\n",
      "----------\n",
      "Train Loss:  0.3202 Acc:  0.8807\n",
      "Epoch 2529\n",
      "----------\n",
      "Train Loss:  0.3751 Acc:  0.8486\n",
      "Epoch 2629\n",
      "----------\n",
      "Train Loss:  0.1756 Acc:  0.9358\n",
      "Epoch 2729\n",
      "----------\n",
      "Train Loss:  0.2429 Acc:  0.9128\n",
      "Epoch 2829\n",
      "----------\n",
      "Train Loss:  0.4333 Acc:  0.8349\n",
      "Epoch 2929\n",
      "----------\n",
      "Train Loss:  0.3484 Acc:  0.8486\n",
      "Training complete in 7m 9s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Linear(num_in_ft, 2)\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.9)\n",
    "\n",
    "resnet101_AB40_SYN = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN,\n",
    "                                 num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049\n",
      "----------\n",
      "Train Loss:  1.6838 Acc:  0.5092\n",
      "Epoch 149\n",
      "----------\n",
      "Train Loss:  0.9357 Acc:  0.5229\n",
      "Epoch 249\n",
      "----------\n",
      "Train Loss:  0.9375 Acc:  0.4679\n",
      "Epoch 349\n",
      "----------\n",
      "Train Loss:  0.7372 Acc:  0.5550\n",
      "Epoch 449\n",
      "----------\n",
      "Train Loss:  0.7356 Acc:  0.5550\n",
      "Epoch 549\n",
      "----------\n",
      "Train Loss:  0.7229 Acc:  0.5367\n",
      "Epoch 649\n",
      "----------\n",
      "Train Loss:  0.6775 Acc:  0.6009\n",
      "Epoch 749\n",
      "----------\n",
      "Train Loss:  0.6712 Acc:  0.5872\n",
      "Epoch 849\n",
      "----------\n",
      "Train Loss:  0.6403 Acc:  0.6330\n",
      "Epoch 949\n",
      "----------\n",
      "Train Loss:  0.6886 Acc:  0.6147\n",
      "Epoch 1049\n",
      "----------\n",
      "Train Loss:  0.6542 Acc:  0.5917\n",
      "Epoch 1149\n",
      "----------\n",
      "Train Loss:  0.6588 Acc:  0.6009\n",
      "Epoch 1249\n",
      "----------\n",
      "Train Loss:  0.6568 Acc:  0.6560\n",
      "Epoch 1349\n",
      "----------\n",
      "Train Loss:  0.6405 Acc:  0.5688\n",
      "Epoch 1449\n",
      "----------\n",
      "Train Loss:  0.7651 Acc:  0.5688\n",
      "Epoch 1549\n",
      "----------\n",
      "Train Loss:  0.6688 Acc:  0.5872\n",
      "Epoch 1649\n",
      "----------\n",
      "Train Loss:  0.6503 Acc:  0.6101\n",
      "Epoch 1749\n",
      "----------\n",
      "Train Loss:  0.6712 Acc:  0.6009\n",
      "Epoch 1849\n",
      "----------\n",
      "Train Loss:  0.6689 Acc:  0.6330\n",
      "Epoch 1949\n",
      "----------\n",
      "Train Loss:  0.4857 Acc:  0.8073\n",
      "Epoch 2049\n",
      "----------\n",
      "Train Loss:  0.5461 Acc:  0.7477\n",
      "Epoch 2149\n",
      "----------\n",
      "Train Loss:  0.5847 Acc:  0.6697\n",
      "Epoch 2249\n",
      "----------\n",
      "Train Loss:  0.3578 Acc:  0.8303\n",
      "Epoch 2349\n",
      "----------\n",
      "Train Loss:  0.5130 Acc:  0.7615\n",
      "Epoch 2449\n",
      "----------\n",
      "Train Loss:  0.4891 Acc:  0.7706\n",
      "Epoch 2549\n",
      "----------\n",
      "Train Loss:  0.4256 Acc:  0.8165\n",
      "Epoch 2649\n",
      "----------\n",
      "Train Loss:  0.3608 Acc:  0.8532\n",
      "Epoch 2749\n",
      "----------\n",
      "Train Loss:  0.3073 Acc:  0.8761\n",
      "Epoch 2849\n",
      "----------\n",
      "Train Loss:  0.2574 Acc:  0.9220\n",
      "Epoch 2949\n",
      "----------\n",
      "Train Loss:  0.1802 Acc:  0.9404\n",
      "Epoch 3049\n",
      "----------\n",
      "Train Loss:  0.1474 Acc:  0.9587\n",
      "Epoch 3149\n",
      "----------\n",
      "Train Loss:  0.1122 Acc:  0.9450\n",
      "Epoch 3249\n",
      "----------\n",
      "Train Loss:  0.3369 Acc:  0.8761\n",
      "Epoch 3349\n",
      "----------\n",
      "Train Loss:  0.3558 Acc:  0.8578\n",
      "Epoch 3449\n",
      "----------\n",
      "Train Loss:  0.2281 Acc:  0.8945\n",
      "Epoch 3549\n",
      "----------\n",
      "Train Loss:  0.1206 Acc:  0.9541\n",
      "Epoch 3649\n",
      "----------\n",
      "Train Loss:  0.1751 Acc:  0.9174\n",
      "Epoch 3749\n",
      "----------\n",
      "Train Loss:  0.3940 Acc:  0.8578\n",
      "Epoch 3849\n",
      "----------\n",
      "Train Loss:  0.3446 Acc:  0.8807\n",
      "Epoch 3949\n",
      "----------\n",
      "Train Loss:  0.1416 Acc:  0.9495\n",
      "Epoch 4049\n",
      "----------\n",
      "Train Loss:  0.1346 Acc:  0.9633\n",
      "Epoch 4149\n",
      "----------\n",
      "Train Loss:  0.2395 Acc:  0.9174\n",
      "Epoch 4249\n",
      "----------\n",
      "Train Loss:  0.2789 Acc:  0.9037\n",
      "Epoch 4349\n",
      "----------\n",
      "Train Loss:  0.1512 Acc:  0.9495\n",
      "Epoch 4449\n",
      "----------\n",
      "Train Loss:  0.0339 Acc:  0.9908\n",
      "Epoch 4549\n",
      "----------\n",
      "Train Loss:  0.0300 Acc:  0.9908\n",
      "Epoch 4649\n",
      "----------\n",
      "Train Loss:  0.1175 Acc:  0.9587\n",
      "Epoch 4749\n",
      "----------\n",
      "Train Loss:  0.1186 Acc:  0.9725\n",
      "Epoch 4849\n",
      "----------\n",
      "Train Loss:  0.1569 Acc:  0.9450\n",
      "Epoch 4949\n",
      "----------\n",
      "Train Loss:  0.0328 Acc:  0.9908\n",
      "Training complete in 12m 32s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Linear(num_in_ft, 2)\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.9)\n",
    "\n",
    "resnet101_AB40_SYN = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN,\n",
    "                                 num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, acc = test(resnet101_AB40_SYN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5851, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StepLR (Step = 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049\n",
      "----------\n",
      "Train Loss:  1.4796 Acc:  0.4954\n",
      "Epoch 149\n",
      "----------\n",
      "Train Loss:  1.1388 Acc:  0.5459\n",
      "Epoch 249\n",
      "----------\n",
      "Train Loss:  0.8907 Acc:  0.5321\n",
      "Epoch 349\n",
      "----------\n",
      "Train Loss:  0.8053 Acc:  0.5321\n",
      "Epoch 449\n",
      "----------\n",
      "Train Loss:  0.6972 Acc:  0.5550\n",
      "Epoch 549\n",
      "----------\n",
      "Train Loss:  0.7075 Acc:  0.5917\n",
      "Epoch 649\n",
      "----------\n",
      "Train Loss:  0.7342 Acc:  0.5183\n",
      "Epoch 749\n",
      "----------\n",
      "Train Loss:  0.7090 Acc:  0.5642\n",
      "Epoch 849\n",
      "----------\n",
      "Train Loss:  0.7481 Acc:  0.5963\n",
      "Epoch 949\n",
      "----------\n",
      "Train Loss:  0.7366 Acc:  0.4908\n",
      "Epoch 1049\n",
      "----------\n",
      "Train Loss:  0.6762 Acc:  0.5734\n",
      "Epoch 1149\n",
      "----------\n",
      "Train Loss:  0.6648 Acc:  0.5642\n",
      "Epoch 1249\n",
      "----------\n",
      "Train Loss:  0.6885 Acc:  0.5917\n",
      "Epoch 1349\n",
      "----------\n",
      "Train Loss:  0.6698 Acc:  0.6009\n",
      "Epoch 1449\n",
      "----------\n",
      "Train Loss:  0.6267 Acc:  0.6697\n",
      "Epoch 1549\n",
      "----------\n",
      "Train Loss:  0.6387 Acc:  0.6101\n",
      "Epoch 1649\n",
      "----------\n",
      "Train Loss:  0.6200 Acc:  0.6606\n",
      "Epoch 1749\n",
      "----------\n",
      "Train Loss:  0.5852 Acc:  0.7064\n",
      "Epoch 1849\n",
      "----------\n",
      "Train Loss:  0.6320 Acc:  0.6651\n",
      "Epoch 1949\n",
      "----------\n",
      "Train Loss:  0.6536 Acc:  0.6789\n",
      "Epoch 2049\n",
      "----------\n",
      "Train Loss:  0.5485 Acc:  0.7294\n",
      "Epoch 2149\n",
      "----------\n",
      "Train Loss:  0.5394 Acc:  0.7661\n",
      "Epoch 2249\n",
      "----------\n",
      "Train Loss:  0.5645 Acc:  0.7385\n",
      "Epoch 2349\n",
      "----------\n",
      "Train Loss:  0.5084 Acc:  0.7752\n",
      "Epoch 2449\n",
      "----------\n",
      "Train Loss:  0.4854 Acc:  0.7752\n",
      "Epoch 2549\n",
      "----------\n",
      "Train Loss:  0.4601 Acc:  0.8073\n",
      "Epoch 2649\n",
      "----------\n",
      "Train Loss:  0.4774 Acc:  0.7752\n",
      "Epoch 2749\n",
      "----------\n",
      "Train Loss:  0.3860 Acc:  0.8440\n",
      "Epoch 2849\n",
      "----------\n",
      "Train Loss:  0.5426 Acc:  0.7523\n",
      "Epoch 2949\n",
      "----------\n",
      "Train Loss:  0.3896 Acc:  0.8578\n",
      "Epoch 3049\n",
      "----------\n",
      "Train Loss:  0.2763 Acc:  0.8945\n",
      "Epoch 3149\n",
      "----------\n",
      "Train Loss:  0.2161 Acc:  0.9083\n",
      "Epoch 3249\n",
      "----------\n",
      "Train Loss:  0.3676 Acc:  0.8624\n",
      "Epoch 3349\n",
      "----------\n",
      "Train Loss:  0.3774 Acc:  0.8257\n",
      "Epoch 3449\n",
      "----------\n",
      "Train Loss:  0.2305 Acc:  0.9128\n",
      "Epoch 3549\n",
      "----------\n",
      "Train Loss:  0.2544 Acc:  0.8945\n",
      "Epoch 3649\n",
      "----------\n",
      "Train Loss:  0.2585 Acc:  0.9083\n",
      "Epoch 3749\n",
      "----------\n",
      "Train Loss:  0.2490 Acc:  0.8945\n",
      "Epoch 3849\n",
      "----------\n",
      "Train Loss:  0.1975 Acc:  0.9128\n",
      "Epoch 3949\n",
      "----------\n",
      "Train Loss:  0.1548 Acc:  0.9495\n",
      "Epoch 4049\n",
      "----------\n",
      "Train Loss:  0.2501 Acc:  0.9037\n",
      "Epoch 4149\n",
      "----------\n",
      "Train Loss:  0.1096 Acc:  0.9633\n",
      "Epoch 4249\n",
      "----------\n",
      "Train Loss:  0.0613 Acc:  0.9771\n",
      "Epoch 4349\n",
      "----------\n",
      "Train Loss:  0.0522 Acc:  0.9771\n",
      "Epoch 4449\n",
      "----------\n",
      "Train Loss:  0.1505 Acc:  0.9587\n",
      "Epoch 4549\n",
      "----------\n",
      "Train Loss:  0.0640 Acc:  0.9725\n",
      "Epoch 4649\n",
      "----------\n",
      "Train Loss:  0.0579 Acc:  0.9679\n",
      "Epoch 4749\n",
      "----------\n",
      "Train Loss:  0.0990 Acc:  0.9679\n",
      "Epoch 4849\n",
      "----------\n",
      "Train Loss:  0.0705 Acc:  0.9679\n",
      "Epoch 4949\n",
      "----------\n",
      "Train Loss:  0.2134 Acc:  0.9312\n",
      "Training complete in 13m 2s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Linear(num_in_ft, 2)\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.9)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer_AB40_SYN,\n",
    "                                step_size=10, gamma=0.8)\n",
    "\n",
    "resnet101_AB40_SYN = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN, scheduler,\n",
    "                                 num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,acc_test = test(resnet101_AB40_SYN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5957, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,acc_train = test(resnet101_AB40_SYN,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9541, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StepLR (Step=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049\n",
      "----------\n",
      "Train Loss:  1.2672 Acc:  0.5505\n",
      "Epoch 149\n",
      "----------\n",
      "Train Loss:  1.1822 Acc:  0.5000\n",
      "Epoch 249\n",
      "----------\n",
      "Train Loss:  1.4839 Acc:  0.4771\n",
      "Epoch 349\n",
      "----------\n",
      "Train Loss:  0.9365 Acc:  0.4633\n",
      "Epoch 449\n",
      "----------\n",
      "Train Loss:  0.7102 Acc:  0.5596\n",
      "Epoch 549\n",
      "----------\n",
      "Train Loss:  0.7386 Acc:  0.5229\n",
      "Epoch 649\n",
      "----------\n",
      "Train Loss:  0.7082 Acc:  0.5229\n",
      "Epoch 749\n",
      "----------\n",
      "Train Loss:  0.6792 Acc:  0.5734\n",
      "Epoch 849\n",
      "----------\n",
      "Train Loss:  0.7136 Acc:  0.5688\n",
      "Epoch 949\n",
      "----------\n",
      "Train Loss:  0.6581 Acc:  0.5780\n",
      "Epoch 1049\n",
      "----------\n",
      "Train Loss:  0.6623 Acc:  0.6009\n",
      "Epoch 1149\n",
      "----------\n",
      "Train Loss:  0.6495 Acc:  0.6239\n",
      "Epoch 1249\n",
      "----------\n",
      "Train Loss:  0.6373 Acc:  0.6193\n",
      "Epoch 1349\n",
      "----------\n",
      "Train Loss:  0.7053 Acc:  0.5642\n",
      "Epoch 1449\n",
      "----------\n",
      "Train Loss:  0.6294 Acc:  0.6239\n",
      "Epoch 1549\n",
      "----------\n",
      "Train Loss:  0.5254 Acc:  0.7615\n",
      "Epoch 1649\n",
      "----------\n",
      "Train Loss:  0.4941 Acc:  0.7339\n",
      "Epoch 1749\n",
      "----------\n",
      "Train Loss:  0.5976 Acc:  0.6881\n",
      "Epoch 1849\n",
      "----------\n",
      "Train Loss:  0.6546 Acc:  0.5963\n",
      "Epoch 1949\n",
      "----------\n",
      "Train Loss:  0.5857 Acc:  0.6560\n",
      "Epoch 2049\n",
      "----------\n",
      "Train Loss:  0.6084 Acc:  0.6560\n",
      "Epoch 2149\n",
      "----------\n",
      "Train Loss:  0.4954 Acc:  0.7477\n",
      "Epoch 2249\n",
      "----------\n",
      "Train Loss:  0.4420 Acc:  0.8119\n",
      "Epoch 2349\n",
      "----------\n",
      "Train Loss:  0.4058 Acc:  0.8165\n",
      "Epoch 2449\n",
      "----------\n",
      "Train Loss:  0.4889 Acc:  0.7661\n",
      "Epoch 2549\n",
      "----------\n",
      "Train Loss:  0.3831 Acc:  0.7982\n",
      "Epoch 2649\n",
      "----------\n",
      "Train Loss:  0.3319 Acc:  0.8532\n",
      "Epoch 2749\n",
      "----------\n",
      "Train Loss:  0.2113 Acc:  0.9128\n",
      "Epoch 2849\n",
      "----------\n",
      "Train Loss:  0.2505 Acc:  0.8899\n",
      "Epoch 2949\n",
      "----------\n",
      "Train Loss:  0.3814 Acc:  0.8349\n",
      "Epoch 3049\n",
      "----------\n",
      "Train Loss:  0.2287 Acc:  0.9220\n",
      "Epoch 3149\n",
      "----------\n",
      "Train Loss:  0.1369 Acc:  0.9541\n",
      "Epoch 3249\n",
      "----------\n",
      "Train Loss:  0.1153 Acc:  0.9587\n",
      "Epoch 3349\n",
      "----------\n",
      "Train Loss:  0.1498 Acc:  0.9358\n",
      "Epoch 3449\n",
      "----------\n",
      "Train Loss:  0.0882 Acc:  0.9633\n",
      "Epoch 3549\n",
      "----------\n",
      "Train Loss:  0.0511 Acc:  0.9725\n",
      "Epoch 3649\n",
      "----------\n",
      "Train Loss:  0.0330 Acc:  0.9954\n",
      "Epoch 3749\n",
      "----------\n",
      "Train Loss:  0.0528 Acc:  0.9771\n",
      "Epoch 3849\n",
      "----------\n",
      "Train Loss:  0.0869 Acc:  0.9679\n",
      "Epoch 3949\n",
      "----------\n",
      "Train Loss:  0.0345 Acc:  0.9954\n",
      "Epoch 4049\n",
      "----------\n",
      "Train Loss:  0.0296 Acc:  0.9954\n",
      "Epoch 4149\n",
      "----------\n",
      "Train Loss:  0.1236 Acc:  0.9541\n",
      "Epoch 4249\n",
      "----------\n",
      "Train Loss:  0.0443 Acc:  0.9908\n",
      "Epoch 4349\n",
      "----------\n",
      "Train Loss:  0.0770 Acc:  0.9771\n",
      "Epoch 4449\n",
      "----------\n",
      "Train Loss:  0.1262 Acc:  0.9587\n",
      "Epoch 4549\n",
      "----------\n",
      "Train Loss:  0.0602 Acc:  0.9725\n",
      "Epoch 4649\n",
      "----------\n",
      "Train Loss:  0.0454 Acc:  0.9817\n",
      "Epoch 4749\n",
      "----------\n",
      "Train Loss:  0.0062 Acc:  1.0000\n",
      "Epoch 4849\n",
      "----------\n",
      "Train Loss:  0.0412 Acc:  0.9862\n",
      "Epoch 4949\n",
      "----------\n",
      "Train Loss:  0.0270 Acc:  0.9862\n",
      "Training complete in 12m 59s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Linear(num_in_ft, 2)\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.9)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer_AB40_SYN,\n",
    "                                step_size=5, gamma=0.8)\n",
    "\n",
    "resnet101_AB40_SYN = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN, scheduler,\n",
    "                                 num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7660, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, acc_test_5 = test(resnet101_AB40_SYN)\n",
    "acc_test_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7660, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, acc_test_6 = test(model_AB40_SYN)\n",
    "acc_test_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, acc_train_5 = test(resnet101_AB40_SYN, train_loader)\n",
    "acc_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, acc_train_6 = test(model_AB40_SYN,train_loader)\n",
    "acc_train_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(scheduler.last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  StepLR (step size=5) with l2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 = 0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049\n",
      "----------\n",
      "Train Loss:  1.0532 Acc:  0.5321\n",
      "Epoch 149\n",
      "----------\n",
      "Train Loss:  0.8473 Acc:  0.5413\n",
      "Epoch 249\n",
      "----------\n",
      "Train Loss:  0.8572 Acc:  0.5229\n",
      "Epoch 349\n",
      "----------\n",
      "Train Loss:  0.8427 Acc:  0.4908\n",
      "Epoch 449\n",
      "----------\n",
      "Train Loss:  0.8567 Acc:  0.5000\n",
      "Epoch 549\n",
      "----------\n",
      "Train Loss:  0.7037 Acc:  0.5550\n",
      "Epoch 649\n",
      "----------\n",
      "Train Loss:  0.6694 Acc:  0.6147\n",
      "Epoch 749\n",
      "----------\n",
      "Train Loss:  0.6595 Acc:  0.5596\n",
      "Epoch 849\n",
      "----------\n",
      "Train Loss:  0.6402 Acc:  0.6376\n",
      "Epoch 949\n",
      "----------\n",
      "Train Loss:  0.6334 Acc:  0.5917\n",
      "Epoch 1049\n",
      "----------\n",
      "Train Loss:  0.6613 Acc:  0.6193\n",
      "Epoch 1149\n",
      "----------\n",
      "Train Loss:  0.6117 Acc:  0.6697\n",
      "Epoch 1249\n",
      "----------\n",
      "Train Loss:  0.6250 Acc:  0.6376\n",
      "Epoch 1349\n",
      "----------\n",
      "Train Loss:  0.5830 Acc:  0.6697\n",
      "Epoch 1449\n",
      "----------\n",
      "Train Loss:  0.5315 Acc:  0.7385\n",
      "Epoch 1549\n",
      "----------\n",
      "Train Loss:  0.3709 Acc:  0.8165\n",
      "Epoch 1649\n",
      "----------\n",
      "Train Loss:  0.3944 Acc:  0.8303\n",
      "Epoch 1749\n",
      "----------\n",
      "Train Loss:  0.4937 Acc:  0.8028\n",
      "Epoch 1849\n",
      "----------\n",
      "Train Loss:  0.3068 Acc:  0.8716\n",
      "Epoch 1949\n",
      "----------\n",
      "Train Loss:  0.4268 Acc:  0.8394\n",
      "Epoch 2049\n",
      "----------\n",
      "Train Loss:  0.5466 Acc:  0.7110\n",
      "Epoch 2149\n",
      "----------\n",
      "Train Loss:  0.3026 Acc:  0.9220\n",
      "Epoch 2249\n",
      "----------\n",
      "Train Loss:  0.1876 Acc:  0.9358\n",
      "Epoch 2349\n",
      "----------\n",
      "Train Loss:  0.1697 Acc:  0.9404\n",
      "Epoch 2449\n",
      "----------\n",
      "Train Loss:  0.3253 Acc:  0.8807\n",
      "Epoch 2549\n",
      "----------\n",
      "Train Loss:  0.3010 Acc:  0.8624\n",
      "Epoch 2649\n",
      "----------\n",
      "Train Loss:  0.1633 Acc:  0.9541\n",
      "Epoch 2749\n",
      "----------\n",
      "Train Loss:  0.1395 Acc:  0.9633\n",
      "Epoch 2849\n",
      "----------\n",
      "Train Loss:  0.2302 Acc:  0.9174\n",
      "Epoch 2949\n",
      "----------\n",
      "Train Loss:  0.1971 Acc:  0.9174\n",
      "Epoch 3049\n",
      "----------\n",
      "Train Loss:  0.1858 Acc:  0.9266\n",
      "Epoch 3149\n",
      "----------\n",
      "Train Loss:  0.0700 Acc:  0.9771\n",
      "Epoch 3249\n",
      "----------\n",
      "Train Loss:  0.1046 Acc:  0.9450\n",
      "Epoch 3349\n",
      "----------\n",
      "Train Loss:  0.1141 Acc:  0.9541\n",
      "Epoch 3449\n",
      "----------\n",
      "Train Loss:  0.1186 Acc:  0.9541\n",
      "Epoch 3549\n",
      "----------\n",
      "Train Loss:  0.0830 Acc:  0.9633\n",
      "Epoch 3649\n",
      "----------\n",
      "Train Loss:  0.0330 Acc:  0.9954\n",
      "Epoch 3749\n",
      "----------\n",
      "Train Loss:  0.0250 Acc:  0.9954\n",
      "Epoch 3849\n",
      "----------\n",
      "Train Loss:  0.0159 Acc:  0.9954\n",
      "Epoch 3949\n",
      "----------\n",
      "Train Loss:  0.0250 Acc:  0.9908\n",
      "Epoch 4049\n",
      "----------\n",
      "Train Loss:  0.0386 Acc:  0.9862\n",
      "Epoch 4149\n",
      "----------\n",
      "Train Loss:  0.0148 Acc:  1.0000\n",
      "Epoch 4249\n",
      "----------\n",
      "Train Loss:  0.0140 Acc:  1.0000\n",
      "Epoch 4349\n",
      "----------\n",
      "Train Loss:  0.0079 Acc:  1.0000\n",
      "Epoch 4449\n",
      "----------\n",
      "Train Loss:  0.0112 Acc:  1.0000\n",
      "Epoch 4549\n",
      "----------\n",
      "Train Loss:  0.0094 Acc:  1.0000\n",
      "Epoch 4649\n",
      "----------\n",
      "Train Loss:  0.0054 Acc:  1.0000\n",
      "Epoch 4749\n",
      "----------\n",
      "Train Loss:  0.0043 Acc:  1.0000\n",
      "Epoch 4849\n",
      "----------\n",
      "Train Loss:  0.0082 Acc:  1.0000\n",
      "Epoch 4949\n",
      "----------\n",
      "Train Loss:  0.0088 Acc:  1.0000\n",
      "Training complete in 14m 3s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Linear(num_in_ft, 2)\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer_AB40_SYN,\n",
    "                                step_size=5, gamma=0.8)\n",
    "\n",
    "resnet101_AB40_SYN = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN, scheduler,\n",
    "                                 num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7021, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, acc_test_1 = test(resnet101_AB40_SYN)\n",
    "acc_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, acc_train_1 = test(resnet101_AB40_SYN,train_loader)\n",
    "acc_train_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049\n",
      "----------\n",
      "Train Loss:  1.3814 Acc:  0.5138\n",
      "Epoch 149\n",
      "----------\n",
      "Train Loss:  1.1416 Acc:  0.5046\n",
      "Epoch 249\n",
      "----------\n",
      "Train Loss:  0.7246 Acc:  0.4862\n",
      "Epoch 349\n",
      "----------\n",
      "Train Loss:  0.7150 Acc:  0.4725\n",
      "Epoch 449\n",
      "----------\n",
      "Train Loss:  0.7058 Acc:  0.5092\n",
      "Epoch 549\n",
      "----------\n",
      "Train Loss:  0.6984 Acc:  0.4908\n",
      "Epoch 649\n",
      "----------\n",
      "Train Loss:  0.6988 Acc:  0.5138\n",
      "Epoch 749\n",
      "----------\n",
      "Train Loss:  0.6960 Acc:  0.4495\n",
      "Epoch 849\n",
      "----------\n",
      "Train Loss:  0.6940 Acc:  0.4587\n",
      "Epoch 949\n",
      "----------\n",
      "Train Loss:  0.6943 Acc:  0.4954\n",
      "Epoch 1049\n",
      "----------\n",
      "Train Loss:  0.6935 Acc:  0.5092\n",
      "Epoch 1149\n",
      "----------\n",
      "Train Loss:  0.6939 Acc:  0.5092\n",
      "Epoch 1249\n",
      "----------\n",
      "Train Loss:  0.6932 Acc:  0.5092\n",
      "Epoch 1349\n",
      "----------\n",
      "Train Loss:  0.6937 Acc:  0.5092\n",
      "Epoch 1449\n",
      "----------\n",
      "Train Loss:  0.6936 Acc:  0.5092\n",
      "Epoch 1549\n",
      "----------\n",
      "Train Loss:  0.6936 Acc:  0.5092\n",
      "Epoch 1649\n",
      "----------\n",
      "Train Loss:  0.6934 Acc:  0.5092\n",
      "Epoch 1749\n",
      "----------\n",
      "Train Loss:  0.6940 Acc:  0.5092\n",
      "Epoch 1849\n",
      "----------\n",
      "Train Loss:  0.6935 Acc:  0.5092\n",
      "Epoch 1949\n",
      "----------\n",
      "Train Loss:  0.6941 Acc:  0.5092\n",
      "Epoch 2049\n",
      "----------\n",
      "Train Loss:  0.6935 Acc:  0.5092\n",
      "Epoch 2149\n",
      "----------\n",
      "Train Loss:  0.6933 Acc:  0.5092\n",
      "Epoch 2249\n",
      "----------\n",
      "Train Loss:  0.6944 Acc:  0.5092\n",
      "Epoch 2349\n",
      "----------\n",
      "Train Loss:  0.6933 Acc:  0.5092\n",
      "Epoch 2449\n",
      "----------\n",
      "Train Loss:  0.6935 Acc:  0.4908\n",
      "Epoch 2549\n",
      "----------\n",
      "Train Loss:  0.6947 Acc:  0.5092\n",
      "Epoch 2649\n",
      "----------\n",
      "Train Loss:  0.6933 Acc:  0.5092\n",
      "Epoch 2749\n",
      "----------\n",
      "Train Loss:  0.6932 Acc:  0.5092\n",
      "Epoch 2849\n",
      "----------\n",
      "Train Loss:  0.6931 Acc:  0.5092\n",
      "Epoch 2949\n",
      "----------\n",
      "Train Loss:  0.6933 Acc:  0.5092\n",
      "Epoch 3049\n",
      "----------\n",
      "Train Loss:  0.6931 Acc:  0.5092\n",
      "Epoch 3149\n",
      "----------\n",
      "Train Loss:  0.6931 Acc:  0.5092\n",
      "Epoch 3249\n",
      "----------\n",
      "Train Loss:  0.6931 Acc:  0.5092\n",
      "Epoch 3349\n",
      "----------\n",
      "Train Loss:  0.6936 Acc:  0.5092\n",
      "Epoch 3449\n",
      "----------\n",
      "Train Loss:  0.6933 Acc:  0.5092\n",
      "Epoch 3549\n",
      "----------\n",
      "Train Loss:  0.6932 Acc:  0.5092\n",
      "Epoch 3649\n",
      "----------\n",
      "Train Loss:  0.6931 Acc:  0.5092\n",
      "Epoch 3749\n",
      "----------\n",
      "Train Loss:  0.6933 Acc:  0.5092\n",
      "Epoch 3849\n",
      "----------\n",
      "Train Loss:  0.6932 Acc:  0.5092\n",
      "Epoch 3949\n",
      "----------\n",
      "Train Loss:  0.6934 Acc:  0.5092\n",
      "Epoch 4049\n",
      "----------\n",
      "Train Loss:  0.6931 Acc:  0.5092\n",
      "Epoch 4149\n",
      "----------\n",
      "Train Loss:  0.6931 Acc:  0.5092\n",
      "Epoch 4249\n",
      "----------\n",
      "Train Loss:  0.6931 Acc:  0.5092\n",
      "Epoch 4349\n",
      "----------\n",
      "Train Loss:  0.6932 Acc:  0.5092\n",
      "Epoch 4449\n",
      "----------\n",
      "Train Loss:  0.6932 Acc:  0.5092\n",
      "Epoch 4549\n",
      "----------\n",
      "Train Loss:  0.6932 Acc:  0.5092\n",
      "Epoch 4649\n",
      "----------\n",
      "Train Loss:  0.6932 Acc:  0.5092\n",
      "Epoch 4749\n",
      "----------\n",
      "Train Loss:  0.6931 Acc:  0.5092\n",
      "Epoch 4849\n",
      "----------\n",
      "Train Loss:  0.6931 Acc:  0.5092\n",
      "Epoch 4949\n",
      "----------\n",
      "Train Loss:  0.6931 Acc:  0.5092\n",
      "Training complete in 14m 8s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Linear(num_in_ft, 2)\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.9, weight_decay=0.1)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer_AB40_SYN,\n",
    "                                step_size=5, gamma=0.8)\n",
    "\n",
    "resnet101_AB40_SYN = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN, scheduler,\n",
    "                                 num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5106, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, acc_test_2 = test(resnet101_AB40_SYN)\n",
    "acc_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5092, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, acc_train_2 = test(resnet101_AB40_SYN,train_loader)\n",
    "acc_train_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 = 0.05 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049\n",
      "----------\n",
      "Train Loss:  1.3130 Acc:  0.5046\n",
      "Epoch 149\n",
      "----------\n",
      "Train Loss:  1.1142 Acc:  0.4633\n",
      "Epoch 249\n",
      "----------\n",
      "Train Loss:  1.0775 Acc:  0.5138\n",
      "Epoch 349\n",
      "----------\n",
      "Train Loss:  0.7925 Acc:  0.4771\n",
      "Epoch 449\n",
      "----------\n",
      "Train Loss:  0.7335 Acc:  0.4725\n",
      "Epoch 549\n",
      "----------\n",
      "Train Loss:  0.6897 Acc:  0.5413\n",
      "Epoch 649\n",
      "----------\n",
      "Train Loss:  0.7016 Acc:  0.4862\n",
      "Epoch 749\n",
      "----------\n",
      "Train Loss:  0.6909 Acc:  0.5000\n",
      "Epoch 849\n",
      "----------\n",
      "Train Loss:  0.6855 Acc:  0.5321\n",
      "Epoch 949\n",
      "----------\n",
      "Train Loss:  0.6800 Acc:  0.5826\n",
      "Epoch 1049\n",
      "----------\n",
      "Train Loss:  0.6930 Acc:  0.4908\n",
      "Epoch 1149\n",
      "----------\n",
      "Train Loss:  0.6570 Acc:  0.5780\n",
      "Epoch 1249\n",
      "----------\n",
      "Train Loss:  0.6658 Acc:  0.5505\n",
      "Epoch 1349\n",
      "----------\n",
      "Train Loss:  0.6851 Acc:  0.5688\n",
      "Epoch 1449\n",
      "----------\n",
      "Train Loss:  0.6782 Acc:  0.5688\n",
      "Epoch 1549\n",
      "----------\n",
      "Train Loss:  0.6653 Acc:  0.5826\n",
      "Epoch 1649\n",
      "----------\n",
      "Train Loss:  0.6455 Acc:  0.6330\n",
      "Epoch 1749\n",
      "----------\n",
      "Train Loss:  0.6264 Acc:  0.6284\n",
      "Epoch 1849\n",
      "----------\n",
      "Train Loss:  0.6635 Acc:  0.5963\n",
      "Epoch 1949\n",
      "----------\n",
      "Train Loss:  0.6897 Acc:  0.5000\n",
      "Epoch 2049\n",
      "----------\n",
      "Train Loss:  0.6652 Acc:  0.5963\n",
      "Epoch 2149\n",
      "----------\n",
      "Train Loss:  0.6578 Acc:  0.6193\n",
      "Epoch 2249\n",
      "----------\n",
      "Train Loss:  0.6480 Acc:  0.6055\n",
      "Epoch 2349\n",
      "----------\n",
      "Train Loss:  0.6507 Acc:  0.6468\n",
      "Epoch 2449\n",
      "----------\n",
      "Train Loss:  0.6013 Acc:  0.6927\n",
      "Epoch 2549\n",
      "----------\n",
      "Train Loss:  0.6256 Acc:  0.6697\n",
      "Epoch 2649\n",
      "----------\n",
      "Train Loss:  0.5906 Acc:  0.7064\n",
      "Epoch 2749\n",
      "----------\n",
      "Train Loss:  0.5629 Acc:  0.7339\n",
      "Epoch 2849\n",
      "----------\n",
      "Train Loss:  0.6114 Acc:  0.6835\n",
      "Epoch 2949\n",
      "----------\n",
      "Train Loss:  0.5698 Acc:  0.6972\n",
      "Epoch 3049\n",
      "----------\n",
      "Train Loss:  0.5263 Acc:  0.7385\n",
      "Epoch 3149\n",
      "----------\n",
      "Train Loss:  0.5321 Acc:  0.7569\n",
      "Epoch 3249\n",
      "----------\n",
      "Train Loss:  0.5189 Acc:  0.7523\n",
      "Epoch 3349\n",
      "----------\n",
      "Train Loss:  0.5224 Acc:  0.7615\n",
      "Epoch 3449\n",
      "----------\n",
      "Train Loss:  0.5172 Acc:  0.7661\n",
      "Epoch 3549\n",
      "----------\n",
      "Train Loss:  0.4586 Acc:  0.8028\n",
      "Epoch 3649\n",
      "----------\n",
      "Train Loss:  0.4185 Acc:  0.8440\n",
      "Epoch 3749\n",
      "----------\n",
      "Train Loss:  0.3806 Acc:  0.8853\n",
      "Epoch 3849\n",
      "----------\n",
      "Train Loss:  0.3716 Acc:  0.8761\n",
      "Epoch 3949\n",
      "----------\n",
      "Train Loss:  0.3542 Acc:  0.8624\n",
      "Epoch 4049\n",
      "----------\n",
      "Train Loss:  0.3585 Acc:  0.8716\n",
      "Epoch 4149\n",
      "----------\n",
      "Train Loss:  0.3095 Acc:  0.9037\n",
      "Epoch 4249\n",
      "----------\n",
      "Train Loss:  0.2655 Acc:  0.9404\n",
      "Epoch 4349\n",
      "----------\n",
      "Train Loss:  0.3005 Acc:  0.8899\n",
      "Epoch 4449\n",
      "----------\n",
      "Train Loss:  0.3349 Acc:  0.8670\n",
      "Epoch 4549\n",
      "----------\n",
      "Train Loss:  0.2355 Acc:  0.9312\n",
      "Epoch 4649\n",
      "----------\n",
      "Train Loss:  0.1981 Acc:  0.9725\n",
      "Epoch 4749\n",
      "----------\n",
      "Train Loss:  0.1667 Acc:  0.9771\n",
      "Epoch 4849\n",
      "----------\n",
      "Train Loss:  0.1714 Acc:  0.9679\n",
      "Epoch 4949\n",
      "----------\n",
      "Train Loss:  0.1342 Acc:  0.9908\n",
      "Training complete in 13m 49s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Linear(num_in_ft, 2)\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.9, weight_decay=0.05)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer_AB40_SYN,\n",
    "                                step_size=5, gamma=0.8)\n",
    "\n",
    "resnet101_AB40_SYN_1 = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN, scheduler,\n",
    "                                 num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,acc_train_3 = test(resnet101_AB40_SYN_1,train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9312, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4787, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,acc_test_3 = test(resnet101_AB40_SYN_1)\n",
    "acc_test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4787, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,acc_test_3 = test(model_AB40_SYN)\n",
    "acc_test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet101_AB40_SYN_1.named_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StepLR with Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\y63qiu/.cache\\torch\\checkpoints\\vgg16-397923af.pth\n",
      "5.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "14.3%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "59.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "98.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with original LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049\n",
      "----------\n",
      "Train Loss:  0.6964 Acc:  0.5046\n",
      "Epoch 149\n",
      "----------\n",
      "Train Loss:  0.7004 Acc:  0.4174\n",
      "Epoch 249\n",
      "----------\n",
      "Train Loss:  0.6859 Acc:  0.5505\n",
      "Epoch 349\n",
      "----------\n",
      "Train Loss:  0.6266 Acc:  0.6651\n",
      "Epoch 449\n",
      "----------\n",
      "Train Loss:  0.6799 Acc:  0.6330\n",
      "Epoch 549\n",
      "----------\n",
      "Train Loss:  0.6804 Acc:  0.5459\n",
      "Epoch 649\n",
      "----------\n",
      "Train Loss:  0.6515 Acc:  0.5734\n",
      "Epoch 749\n",
      "----------\n",
      "Train Loss:  0.6244 Acc:  0.6651\n",
      "Epoch 849\n",
      "----------\n",
      "Train Loss:  0.6201 Acc:  0.6514\n",
      "Epoch 949\n",
      "----------\n",
      "Train Loss:  0.6240 Acc:  0.6972\n",
      "Epoch 1049\n",
      "----------\n",
      "Train Loss:  0.6762 Acc:  0.5917\n",
      "Epoch 1149\n",
      "----------\n",
      "Train Loss:  0.6185 Acc:  0.7110\n",
      "Epoch 1249\n",
      "----------\n",
      "Train Loss:  0.5513 Acc:  0.7798\n",
      "Epoch 1349\n",
      "----------\n",
      "Train Loss:  0.5433 Acc:  0.7706\n",
      "Epoch 1449\n",
      "----------\n",
      "Train Loss:  0.4732 Acc:  0.7661\n",
      "Epoch 1549\n",
      "----------\n",
      "Train Loss:  0.3917 Acc:  0.8532\n",
      "Epoch 1649\n",
      "----------\n",
      "Train Loss:  0.5350 Acc:  0.7798\n",
      "Epoch 1749\n",
      "----------\n",
      "Train Loss:  0.4484 Acc:  0.7982\n",
      "Epoch 1849\n",
      "----------\n",
      "Train Loss:  0.5188 Acc:  0.7477\n",
      "Epoch 1949\n",
      "----------\n",
      "Train Loss:  0.3620 Acc:  0.8991\n",
      "Epoch 2049\n",
      "----------\n",
      "Train Loss:  0.4278 Acc:  0.8028\n",
      "Epoch 2149\n",
      "----------\n",
      "Train Loss:  0.4205 Acc:  0.8486\n",
      "Epoch 2249\n",
      "----------\n",
      "Train Loss:  0.3379 Acc:  0.8761\n",
      "Epoch 2349\n",
      "----------\n",
      "Train Loss:  0.3552 Acc:  0.8670\n",
      "Epoch 2449\n",
      "----------\n",
      "Train Loss:  0.2163 Acc:  0.9083\n",
      "Epoch 2549\n",
      "----------\n",
      "Train Loss:  0.1805 Acc:  0.9404\n",
      "Epoch 2649\n",
      "----------\n",
      "Train Loss:  0.1255 Acc:  0.9633\n",
      "Epoch 2749\n",
      "----------\n",
      "Train Loss:  0.0746 Acc:  0.9771\n",
      "Epoch 2849\n",
      "----------\n",
      "Train Loss:  0.3306 Acc:  0.8945\n",
      "Epoch 2949\n",
      "----------\n",
      "Train Loss:  0.2455 Acc:  0.9128\n",
      "Epoch 3049\n",
      "----------\n",
      "Train Loss:  0.2605 Acc:  0.9312\n",
      "Epoch 3149\n",
      "----------\n",
      "Train Loss:  0.1263 Acc:  0.9679\n",
      "Epoch 3249\n",
      "----------\n",
      "Train Loss:  0.0723 Acc:  0.9862\n",
      "Epoch 3349\n",
      "----------\n",
      "Train Loss:  0.1595 Acc:  0.9450\n",
      "Epoch 3449\n",
      "----------\n",
      "Train Loss:  0.1349 Acc:  0.9541\n",
      "Epoch 3549\n",
      "----------\n",
      "Train Loss:  0.1476 Acc:  0.9495\n",
      "Epoch 3649\n",
      "----------\n",
      "Train Loss:  0.0776 Acc:  0.9771\n",
      "Epoch 3749\n",
      "----------\n",
      "Train Loss:  0.1089 Acc:  0.9679\n",
      "Epoch 3849\n",
      "----------\n",
      "Train Loss:  0.0578 Acc:  0.9862\n",
      "Epoch 3949\n",
      "----------\n",
      "Train Loss:  0.0218 Acc:  1.0000\n",
      "Epoch 4049\n",
      "----------\n",
      "Train Loss:  0.0609 Acc:  0.9771\n",
      "Epoch 4149\n",
      "----------\n",
      "Train Loss:  0.0313 Acc:  0.9908\n",
      "Epoch 4249\n",
      "----------\n",
      "Train Loss:  0.0128 Acc:  1.0000\n",
      "Epoch 4349\n",
      "----------\n",
      "Train Loss:  0.0163 Acc:  0.9954\n",
      "Epoch 4449\n",
      "----------\n",
      "Train Loss:  0.0340 Acc:  0.9908\n",
      "Epoch 4549\n",
      "----------\n",
      "Train Loss:  0.0716 Acc:  0.9771\n",
      "Epoch 4649\n",
      "----------\n",
      "Train Loss:  0.0412 Acc:  0.9908\n",
      "Epoch 4749\n",
      "----------\n",
      "Train Loss:  0.0203 Acc:  0.9954\n",
      "Epoch 4849\n",
      "----------\n",
      "Train Loss:  0.0137 Acc:  0.9954\n",
      "Epoch 4949\n",
      "----------\n",
      "Train Loss:  0.0479 Acc:  0.9862\n",
      "Training complete in 21m 46s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Sequential(\n",
    "                    nn.Linear(num_in_ft, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(512,128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(128,32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(32,2))\n",
    "\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.95)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer_AB40_SYN,\n",
    "                                step_size=5, gamma=0.8)\n",
    "\n",
    "resnet101_AB40_SYN_2 = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN, scheduler,\n",
    "                                 num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,acc_train_5 = test(resnet101_AB40_SYN_2,train_loader)\n",
    "acc_train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7660, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,acc_test_5 = test(resnet101_AB40_SYN_2)\n",
    "acc_test_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with different dropout rate and # of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049\n",
      "----------\n",
      "Train Loss:  0.6940 Acc:  0.5183\n",
      "Epoch 149\n",
      "----------\n",
      "Train Loss:  0.6984 Acc:  0.5459\n",
      "Epoch 249\n",
      "----------\n",
      "Train Loss:  0.6758 Acc:  0.6009\n",
      "Epoch 349\n",
      "----------\n",
      "Train Loss:  0.6853 Acc:  0.5780\n",
      "Epoch 449\n",
      "----------\n",
      "Train Loss:  0.6930 Acc:  0.5917\n",
      "Epoch 549\n",
      "----------\n",
      "Train Loss:  0.7228 Acc:  0.5413\n",
      "Epoch 649\n",
      "----------\n",
      "Train Loss:  0.7059 Acc:  0.5183\n",
      "Epoch 749\n",
      "----------\n",
      "Train Loss:  0.6882 Acc:  0.5826\n",
      "Epoch 849\n",
      "----------\n",
      "Train Loss:  0.6399 Acc:  0.6468\n",
      "Epoch 949\n",
      "----------\n",
      "Train Loss:  0.6836 Acc:  0.5872\n",
      "Epoch 1049\n",
      "----------\n",
      "Train Loss:  0.6974 Acc:  0.5642\n",
      "Epoch 1149\n",
      "----------\n",
      "Train Loss:  0.6379 Acc:  0.6697\n",
      "Epoch 1249\n",
      "----------\n",
      "Train Loss:  0.6538 Acc:  0.6147\n",
      "Epoch 1349\n",
      "----------\n",
      "Train Loss:  0.6532 Acc:  0.6697\n",
      "Epoch 1449\n",
      "----------\n",
      "Train Loss:  0.6748 Acc:  0.5596\n",
      "Epoch 1549\n",
      "----------\n",
      "Train Loss:  0.6451 Acc:  0.6009\n",
      "Epoch 1649\n",
      "----------\n",
      "Train Loss:  0.5854 Acc:  0.7156\n",
      "Epoch 1749\n",
      "----------\n",
      "Train Loss:  0.6518 Acc:  0.7110\n",
      "Epoch 1849\n",
      "----------\n",
      "Train Loss:  0.6845 Acc:  0.6468\n",
      "Epoch 1949\n",
      "----------\n",
      "Train Loss:  0.6350 Acc:  0.6422\n",
      "Epoch 2049\n",
      "----------\n",
      "Train Loss:  0.6509 Acc:  0.7202\n",
      "Epoch 2149\n",
      "----------\n",
      "Train Loss:  0.6043 Acc:  0.6789\n",
      "Epoch 2249\n",
      "----------\n",
      "Train Loss:  0.6520 Acc:  0.6743\n",
      "Epoch 2349\n",
      "----------\n",
      "Train Loss:  0.6307 Acc:  0.6606\n",
      "Epoch 2449\n",
      "----------\n",
      "Train Loss:  0.6113 Acc:  0.7018\n",
      "Epoch 2549\n",
      "----------\n",
      "Train Loss:  0.6235 Acc:  0.6284\n",
      "Epoch 2649\n",
      "----------\n",
      "Train Loss:  0.6069 Acc:  0.6881\n",
      "Epoch 2749\n",
      "----------\n",
      "Train Loss:  0.6041 Acc:  0.7248\n",
      "Epoch 2849\n",
      "----------\n",
      "Train Loss:  0.5332 Acc:  0.7523\n",
      "Epoch 2949\n",
      "----------\n",
      "Train Loss:  0.4779 Acc:  0.8073\n",
      "Epoch 3049\n",
      "----------\n",
      "Train Loss:  0.4886 Acc:  0.7936\n",
      "Epoch 3149\n",
      "----------\n",
      "Train Loss:  0.5074 Acc:  0.7798\n",
      "Epoch 3249\n",
      "----------\n",
      "Train Loss:  0.5158 Acc:  0.7385\n",
      "Epoch 3349\n",
      "----------\n",
      "Train Loss:  0.4376 Acc:  0.8440\n",
      "Epoch 3449\n",
      "----------\n",
      "Train Loss:  0.3405 Acc:  0.8853\n",
      "Epoch 3549\n",
      "----------\n",
      "Train Loss:  0.3572 Acc:  0.8853\n",
      "Epoch 3649\n",
      "----------\n",
      "Train Loss:  0.2979 Acc:  0.9174\n",
      "Epoch 3749\n",
      "----------\n",
      "Train Loss:  0.2431 Acc:  0.9220\n",
      "Epoch 3849\n",
      "----------\n",
      "Train Loss:  0.4033 Acc:  0.8165\n",
      "Epoch 3949\n",
      "----------\n",
      "Train Loss:  0.3249 Acc:  0.8716\n",
      "Epoch 4049\n",
      "----------\n",
      "Train Loss:  0.2461 Acc:  0.9083\n",
      "Epoch 4149\n",
      "----------\n",
      "Train Loss:  0.2415 Acc:  0.9404\n",
      "Epoch 4249\n",
      "----------\n",
      "Train Loss:  0.2742 Acc:  0.9037\n",
      "Epoch 4349\n",
      "----------\n",
      "Train Loss:  0.2180 Acc:  0.9174\n",
      "Epoch 4449\n",
      "----------\n",
      "Train Loss:  0.2021 Acc:  0.9450\n",
      "Epoch 4549\n",
      "----------\n",
      "Train Loss:  0.2684 Acc:  0.9312\n",
      "Epoch 4649\n",
      "----------\n",
      "Train Loss:  0.2019 Acc:  0.9174\n",
      "Epoch 4749\n",
      "----------\n",
      "Train Loss:  0.1766 Acc:  0.9404\n",
      "Epoch 4849\n",
      "----------\n",
      "Train Loss:  0.1428 Acc:  0.9495\n",
      "Epoch 4949\n",
      "----------\n",
      "Train Loss:  0.1257 Acc:  0.9587\n",
      "Training complete in 11m 9s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Sequential(\n",
    "                    nn.Linear(num_in_ft, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(512,128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(128,32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(32,2))\n",
    "\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.95)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer_AB40_SYN,\n",
    "                                step_size=5, gamma=0.8)\n",
    "\n",
    "resnet101_AB40_SYN_2 = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN, scheduler,\n",
    "                                 num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9174, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,acc_train_6 = test(resnet101_AB40_SYN_2,train_loader)\n",
    "acc_train_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6915, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,acc_test_6 = test(resnet101_AB40_SYN_2)\n",
    "acc_test_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 079\n",
      "----------\n",
      "Train Loss:  0.7059 Acc:  0.5229\n",
      "Epoch 179\n",
      "----------\n",
      "Train Loss:  0.6936 Acc:  0.5229\n",
      "Epoch 279\n",
      "----------\n",
      "Train Loss:  0.6977 Acc:  0.5505\n",
      "Epoch 379\n",
      "----------\n",
      "Train Loss:  0.6708 Acc:  0.6330\n",
      "Epoch 479\n",
      "----------\n",
      "Train Loss:  0.7130 Acc:  0.5505\n",
      "Epoch 579\n",
      "----------\n",
      "Train Loss:  0.6421 Acc:  0.6239\n",
      "Epoch 679\n",
      "----------\n",
      "Train Loss:  0.7250 Acc:  0.5688\n",
      "Epoch 779\n",
      "----------\n",
      "Train Loss:  0.6811 Acc:  0.5826\n",
      "Epoch 879\n",
      "----------\n",
      "Train Loss:  0.6864 Acc:  0.5642\n",
      "Epoch 979\n",
      "----------\n",
      "Train Loss:  0.6650 Acc:  0.6468\n",
      "Epoch 1079\n",
      "----------\n",
      "Train Loss:  0.6174 Acc:  0.6835\n",
      "Epoch 1179\n",
      "----------\n",
      "Train Loss:  0.7214 Acc:  0.6101\n",
      "Epoch 1279\n",
      "----------\n",
      "Train Loss:  0.6719 Acc:  0.5872\n",
      "Epoch 1379\n",
      "----------\n",
      "Train Loss:  0.7153 Acc:  0.5734\n",
      "Epoch 1479\n",
      "----------\n",
      "Train Loss:  0.7038 Acc:  0.5092\n",
      "Epoch 1579\n",
      "----------\n",
      "Train Loss:  0.6828 Acc:  0.5826\n",
      "Epoch 1679\n",
      "----------\n",
      "Train Loss:  0.6545 Acc:  0.5963\n",
      "Epoch 1779\n",
      "----------\n",
      "Train Loss:  0.6788 Acc:  0.6376\n",
      "Epoch 1879\n",
      "----------\n",
      "Train Loss:  0.6456 Acc:  0.6147\n",
      "Epoch 1979\n",
      "----------\n",
      "Train Loss:  0.6081 Acc:  0.6927\n",
      "Epoch 2079\n",
      "----------\n",
      "Train Loss:  0.6230 Acc:  0.6881\n",
      "Epoch 2179\n",
      "----------\n",
      "Train Loss:  0.6027 Acc:  0.6514\n",
      "Epoch 2279\n",
      "----------\n",
      "Train Loss:  0.5401 Acc:  0.7798\n",
      "Epoch 2379\n",
      "----------\n",
      "Train Loss:  0.5659 Acc:  0.7661\n",
      "Epoch 2479\n",
      "----------\n",
      "Train Loss:  0.4738 Acc:  0.7890\n",
      "Epoch 2579\n",
      "----------\n",
      "Train Loss:  0.5759 Acc:  0.7936\n",
      "Epoch 2679\n",
      "----------\n",
      "Train Loss:  0.5347 Acc:  0.7615\n",
      "Epoch 2779\n",
      "----------\n",
      "Train Loss:  0.4528 Acc:  0.8119\n",
      "Epoch 2879\n",
      "----------\n",
      "Train Loss:  0.4753 Acc:  0.8165\n",
      "Epoch 2979\n",
      "----------\n",
      "Train Loss:  0.4628 Acc:  0.8073\n",
      "Epoch 3079\n",
      "----------\n",
      "Train Loss:  0.4416 Acc:  0.8211\n",
      "Epoch 3179\n",
      "----------\n",
      "Train Loss:  0.4011 Acc:  0.8394\n",
      "Epoch 3279\n",
      "----------\n",
      "Train Loss:  0.3956 Acc:  0.8440\n",
      "Epoch 3379\n",
      "----------\n",
      "Train Loss:  0.3402 Acc:  0.8807\n",
      "Epoch 3479\n",
      "----------\n",
      "Train Loss:  0.2720 Acc:  0.9037\n",
      "Epoch 3579\n",
      "----------\n",
      "Train Loss:  0.1983 Acc:  0.9450\n",
      "Epoch 3679\n",
      "----------\n",
      "Train Loss:  0.3456 Acc:  0.8899\n",
      "Epoch 3779\n",
      "----------\n",
      "Train Loss:  0.4136 Acc:  0.8349\n",
      "Epoch 3879\n",
      "----------\n",
      "Train Loss:  0.3641 Acc:  0.8394\n",
      "Epoch 3979\n",
      "----------\n",
      "Train Loss:  0.2507 Acc:  0.9312\n",
      "Epoch 4079\n",
      "----------\n",
      "Train Loss:  0.3219 Acc:  0.9083\n",
      "Epoch 4179\n",
      "----------\n",
      "Train Loss:  0.2464 Acc:  0.8899\n",
      "Epoch 4279\n",
      "----------\n",
      "Train Loss:  0.2030 Acc:  0.9174\n",
      "Epoch 4379\n",
      "----------\n",
      "Train Loss:  0.1763 Acc:  0.9358\n",
      "Epoch 4479\n",
      "----------\n",
      "Train Loss:  0.1865 Acc:  0.9450\n",
      "Epoch 4579\n",
      "----------\n",
      "Train Loss:  0.2854 Acc:  0.9174\n",
      "Epoch 4679\n",
      "----------\n",
      "Train Loss:  0.1565 Acc:  0.9450\n",
      "Epoch 4779\n",
      "----------\n",
      "Train Loss:  0.1243 Acc:  0.9679\n",
      "Epoch 4879\n",
      "----------\n",
      "Train Loss:  0.1087 Acc:  0.9771\n",
      "Epoch 4979\n",
      "----------\n",
      "Train Loss:  0.0895 Acc:  0.9679\n",
      "Epoch 5079\n",
      "----------\n",
      "Train Loss:  0.0847 Acc:  0.9817\n",
      "Epoch 5179\n",
      "----------\n",
      "Train Loss:  0.0564 Acc:  0.9908\n",
      "Epoch 5279\n",
      "----------\n",
      "Train Loss:  0.0595 Acc:  0.9862\n",
      "Epoch 5379\n",
      "----------\n",
      "Train Loss:  0.0418 Acc:  0.9908\n",
      "Epoch 5479\n",
      "----------\n",
      "Train Loss:  0.0349 Acc:  0.9954\n",
      "Epoch 5579\n",
      "----------\n",
      "Train Loss:  0.0371 Acc:  0.9908\n",
      "Epoch 5679\n",
      "----------\n",
      "Train Loss:  0.0295 Acc:  0.9954\n",
      "Epoch 5779\n",
      "----------\n",
      "Train Loss:  0.0825 Acc:  0.9862\n",
      "Epoch 5879\n",
      "----------\n",
      "Train Loss:  0.0539 Acc:  0.9954\n",
      "Epoch 5979\n",
      "----------\n",
      "Train Loss:  0.0780 Acc:  0.9771\n",
      "Epoch 6079\n",
      "----------\n",
      "Train Loss:  0.1509 Acc:  0.9862\n",
      "Epoch 6179\n",
      "----------\n",
      "Train Loss:  0.0602 Acc:  0.9817\n",
      "Epoch 6279\n",
      "----------\n",
      "Train Loss:  0.1252 Acc:  0.9725\n",
      "Epoch 6379\n",
      "----------\n",
      "Train Loss:  0.0490 Acc:  0.9817\n",
      "Epoch 6479\n",
      "----------\n",
      "Train Loss:  0.0916 Acc:  0.9771\n",
      "Epoch 6579\n",
      "----------\n",
      "Train Loss:  0.0453 Acc:  0.9862\n",
      "Epoch 6679\n",
      "----------\n",
      "Train Loss:  0.0395 Acc:  0.9954\n",
      "Epoch 6779\n",
      "----------\n",
      "Train Loss:  0.0558 Acc:  0.9862\n",
      "Epoch 6879\n",
      "----------\n",
      "Train Loss:  0.0224 Acc:  1.0000\n",
      "Epoch 6979\n",
      "----------\n",
      "Train Loss:  0.0214 Acc:  0.9954\n",
      "Epoch 7079\n",
      "----------\n",
      "Train Loss:  0.0397 Acc:  0.9908\n",
      "Epoch 7179\n",
      "----------\n",
      "Train Loss:  0.0270 Acc:  1.0000\n",
      "Epoch 7279\n",
      "----------\n",
      "Train Loss:  0.0309 Acc:  0.9954\n",
      "Epoch 7379\n",
      "----------\n",
      "Train Loss:  0.0648 Acc:  0.9817\n",
      "Epoch 7479\n",
      "----------\n",
      "Train Loss:  0.0290 Acc:  0.9954\n",
      "Epoch 7579\n",
      "----------\n",
      "Train Loss:  0.0182 Acc:  0.9954\n",
      "Epoch 7679\n",
      "----------\n",
      "Train Loss:  0.0161 Acc:  1.0000\n",
      "Epoch 7779\n",
      "----------\n",
      "Train Loss:  0.0410 Acc:  0.9954\n",
      "Epoch 7879\n",
      "----------\n",
      "Train Loss:  0.0179 Acc:  1.0000\n",
      "Epoch 7979\n",
      "----------\n",
      "Train Loss:  0.0454 Acc:  0.9908\n",
      "Training complete in 19m 13s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Sequential(\n",
    "                    nn.Linear(num_in_ft, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(512,128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(128,32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(32,2))\n",
    "\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.95)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer_AB40_SYN,\n",
    "                                step_size=5, gamma=0.8)\n",
    "\n",
    "resnet101_AB40_SYN_2 = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN, scheduler,\n",
    "                                 num_epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9495, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,acc_train_7 = test(resnet101_AB40_SYN_2,train_loader)\n",
    "acc_train_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6702, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,acc_test_7 = test(resnet101_AB40_SYN_2)\n",
    "acc_test_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 079\n",
      "----------\n",
      "Train Loss:  0.7045 Acc:  0.4817\n",
      "Epoch 179\n",
      "----------\n",
      "Train Loss:  0.6967 Acc:  0.4862\n",
      "Epoch 279\n",
      "----------\n",
      "Train Loss:  0.7193 Acc:  0.4587\n",
      "Epoch 379\n",
      "----------\n",
      "Train Loss:  0.6896 Acc:  0.5688\n",
      "Epoch 479\n",
      "----------\n",
      "Train Loss:  0.6703 Acc:  0.6284\n",
      "Epoch 579\n",
      "----------\n",
      "Train Loss:  0.6462 Acc:  0.6560\n",
      "Epoch 679\n",
      "----------\n",
      "Train Loss:  0.6779 Acc:  0.5917\n",
      "Epoch 779\n",
      "----------\n",
      "Train Loss:  0.7289 Acc:  0.5413\n",
      "Epoch 879\n",
      "----------\n",
      "Train Loss:  0.7187 Acc:  0.4771\n",
      "Epoch 979\n",
      "----------\n",
      "Train Loss:  0.6596 Acc:  0.6330\n",
      "Epoch 1079\n",
      "----------\n",
      "Train Loss:  0.5994 Acc:  0.7615\n",
      "Epoch 1179\n",
      "----------\n",
      "Train Loss:  0.6318 Acc:  0.7110\n",
      "Epoch 1279\n",
      "----------\n",
      "Train Loss:  0.6526 Acc:  0.6330\n",
      "Epoch 1379\n",
      "----------\n",
      "Train Loss:  0.5644 Acc:  0.7110\n",
      "Epoch 1479\n",
      "----------\n",
      "Train Loss:  0.6178 Acc:  0.7385\n",
      "Epoch 1579\n",
      "----------\n",
      "Train Loss:  0.5667 Acc:  0.7294\n",
      "Epoch 1679\n",
      "----------\n",
      "Train Loss:  0.5801 Acc:  0.7431\n",
      "Epoch 1779\n",
      "----------\n",
      "Train Loss:  0.4825 Acc:  0.8028\n",
      "Epoch 1879\n",
      "----------\n",
      "Train Loss:  0.4575 Acc:  0.8211\n",
      "Epoch 1979\n",
      "----------\n",
      "Train Loss:  0.4133 Acc:  0.8303\n",
      "Epoch 2079\n",
      "----------\n",
      "Train Loss:  0.3553 Acc:  0.8991\n",
      "Epoch 2179\n",
      "----------\n",
      "Train Loss:  0.4499 Acc:  0.7844\n",
      "Epoch 2279\n",
      "----------\n",
      "Train Loss:  0.3316 Acc:  0.9083\n",
      "Epoch 2379\n",
      "----------\n",
      "Train Loss:  0.2948 Acc:  0.8853\n",
      "Epoch 2479\n",
      "----------\n",
      "Train Loss:  0.3117 Acc:  0.8578\n",
      "Epoch 2579\n",
      "----------\n",
      "Train Loss:  0.3113 Acc:  0.8670\n",
      "Epoch 2679\n",
      "----------\n",
      "Train Loss:  0.2629 Acc:  0.9266\n",
      "Epoch 2779\n",
      "----------\n",
      "Train Loss:  0.1776 Acc:  0.9358\n",
      "Epoch 2879\n",
      "----------\n",
      "Train Loss:  0.1351 Acc:  0.9679\n",
      "Epoch 2979\n",
      "----------\n",
      "Train Loss:  0.0933 Acc:  0.9679\n",
      "Epoch 3079\n",
      "----------\n",
      "Train Loss:  0.1243 Acc:  0.9587\n",
      "Epoch 3179\n",
      "----------\n",
      "Train Loss:  0.0976 Acc:  0.9725\n",
      "Epoch 3279\n",
      "----------\n",
      "Train Loss:  0.1615 Acc:  0.9587\n",
      "Epoch 3379\n",
      "----------\n",
      "Train Loss:  0.1423 Acc:  0.9587\n",
      "Epoch 3479\n",
      "----------\n",
      "Train Loss:  0.1532 Acc:  0.9495\n",
      "Epoch 3579\n",
      "----------\n",
      "Train Loss:  0.0938 Acc:  0.9817\n",
      "Epoch 3679\n",
      "----------\n",
      "Train Loss:  0.0770 Acc:  0.9862\n",
      "Epoch 3779\n",
      "----------\n",
      "Train Loss:  0.0251 Acc:  0.9908\n",
      "Epoch 3879\n",
      "----------\n",
      "Train Loss:  0.0462 Acc:  0.9817\n",
      "Epoch 3979\n",
      "----------\n",
      "Train Loss:  0.1435 Acc:  0.9541\n",
      "Epoch 4079\n",
      "----------\n",
      "Train Loss:  0.1177 Acc:  0.9633\n",
      "Epoch 4179\n",
      "----------\n",
      "Train Loss:  0.1524 Acc:  0.9450\n",
      "Epoch 4279\n",
      "----------\n",
      "Train Loss:  0.0815 Acc:  0.9725\n",
      "Epoch 4379\n",
      "----------\n",
      "Train Loss:  0.0372 Acc:  0.9908\n",
      "Epoch 4479\n",
      "----------\n",
      "Train Loss:  0.0158 Acc:  1.0000\n",
      "Epoch 4579\n",
      "----------\n",
      "Train Loss:  0.0121 Acc:  1.0000\n",
      "Epoch 4679\n",
      "----------\n",
      "Train Loss:  0.0120 Acc:  1.0000\n",
      "Epoch 4779\n",
      "----------\n",
      "Train Loss:  0.0077 Acc:  1.0000\n",
      "Epoch 4879\n",
      "----------\n",
      "Train Loss:  0.0104 Acc:  1.0000\n",
      "Epoch 4979\n",
      "----------\n",
      "Train Loss:  0.0441 Acc:  0.9817\n",
      "Epoch 5079\n",
      "----------\n",
      "Train Loss:  0.0189 Acc:  0.9954\n",
      "Epoch 5179\n",
      "----------\n",
      "Train Loss:  0.0113 Acc:  1.0000\n",
      "Epoch 5279\n",
      "----------\n",
      "Train Loss:  0.0100 Acc:  1.0000\n",
      "Epoch 5379\n",
      "----------\n",
      "Train Loss:  0.0100 Acc:  1.0000\n",
      "Epoch 5479\n",
      "----------\n",
      "Train Loss:  0.0442 Acc:  0.9771\n",
      "Epoch 5579\n",
      "----------\n",
      "Train Loss:  0.0076 Acc:  1.0000\n",
      "Epoch 5679\n",
      "----------\n",
      "Train Loss:  0.0240 Acc:  0.9954\n",
      "Epoch 5779\n",
      "----------\n",
      "Train Loss:  0.0516 Acc:  0.9908\n",
      "Epoch 5879\n",
      "----------\n",
      "Train Loss:  0.3233 Acc:  0.9679\n",
      "Epoch 5979\n",
      "----------\n",
      "Train Loss:  0.0392 Acc:  0.9862\n",
      "Epoch 6079\n",
      "----------\n",
      "Train Loss:  0.0447 Acc:  0.9908\n",
      "Epoch 6179\n",
      "----------\n",
      "Train Loss:  0.0267 Acc:  0.9954\n",
      "Epoch 6279\n",
      "----------\n",
      "Train Loss:  0.0171 Acc:  1.0000\n",
      "Epoch 6379\n",
      "----------\n",
      "Train Loss:  0.0453 Acc:  0.9862\n",
      "Epoch 6479\n",
      "----------\n",
      "Train Loss:  0.0196 Acc:  0.9954\n",
      "Epoch 6579\n",
      "----------\n",
      "Train Loss:  0.0171 Acc:  0.9954\n",
      "Epoch 6679\n",
      "----------\n",
      "Train Loss:  0.0097 Acc:  1.0000\n",
      "Epoch 6779\n",
      "----------\n",
      "Train Loss:  0.0130 Acc:  1.0000\n",
      "Epoch 6879\n",
      "----------\n",
      "Train Loss:  0.0104 Acc:  0.9954\n",
      "Epoch 6979\n",
      "----------\n",
      "Train Loss:  0.0375 Acc:  0.9862\n",
      "Epoch 7079\n",
      "----------\n",
      "Train Loss:  0.1868 Acc:  0.9817\n",
      "Epoch 7179\n",
      "----------\n",
      "Train Loss:  0.0146 Acc:  0.9954\n",
      "Epoch 7279\n",
      "----------\n",
      "Train Loss:  0.0205 Acc:  0.9954\n",
      "Epoch 7379\n",
      "----------\n",
      "Train Loss:  0.0115 Acc:  1.0000\n",
      "Epoch 7479\n",
      "----------\n",
      "Train Loss:  0.0127 Acc:  1.0000\n",
      "Epoch 7579\n",
      "----------\n",
      "Train Loss:  0.0100 Acc:  1.0000\n",
      "Epoch 7679\n",
      "----------\n",
      "Train Loss:  0.0134 Acc:  1.0000\n",
      "Epoch 7779\n",
      "----------\n",
      "Train Loss:  0.0075 Acc:  1.0000\n",
      "Epoch 7879\n",
      "----------\n",
      "Train Loss:  0.0056 Acc:  1.0000\n",
      "Epoch 7979\n",
      "----------\n",
      "Train Loss:  0.0069 Acc:  1.0000\n",
      "Training complete in 18m 58s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_AB40_SYN = resnet101(pretrained=True)\n",
    "# The size of each output sample is set to 2\n",
    "num_in_ft = model_AB40_SYN.fc.in_features\n",
    "model_AB40_SYN.fc = nn.Sequential(\n",
    "                    nn.Linear(num_in_ft, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(512,128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.5),\n",
    "                    nn.Linear(128,32),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Linear(32,2))\n",
    "\n",
    "model_AB40_SYN.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_AB40_SYN = torch.optim.SGD(model_AB40_SYN.parameters(),\n",
    "                                     lr=0.01, momentum=0.95)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer_AB40_SYN,\n",
    "                                step_size=5, gamma=0.8)\n",
    "\n",
    "resnet101_AB40_SYN_3 = train_model(model_AB40_SYN, criterion,\n",
    "                                 optimizer_AB40_SYN, scheduler,\n",
    "                                 num_epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9908, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,acc_train_8 = test(resnet101_AB40_SYN_3,train_loader)\n",
    "acc_train_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8085, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,acc_test_8 = test(resnet101_AB40_SYN_3)\n",
    "acc_test_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed random transforms at each epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
    "])\n",
    "transform = {\n",
    "    'train': T.Compose([\n",
    "        T.Resize((224,224)),\n",
    "        T.RandomResizedCrop((224,224), scale=(0.9, 1)),\n",
    "        T.RandomRotation(degree=(-180,180)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5])\n",
    "    ]),\n",
    "    \n",
    "    'test':T.Compose([\n",
    "        T.Resize((224,224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5])\n",
    "    ])\n",
    "}\n",
    "\n",
    "all_dataset = Proteins(root=opt.root, transforms=transform)\n",
    "\n",
    "# split training and test set 7:3\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(all_dataset.labels)),\n",
    "    test_size=0.3,\n",
    "    stratify=all_dataset.labels\n",
    ")\n",
    "\n",
    "train_sampler = SRS(train_idx)\n",
    "test_sampler = SRS(test_idx)\n",
    "train_loader = DataLoader(all_dataset, batch_size=10, sampler=train_sampler)\n",
    "test_loader = DataLoader(all_dataset, batch_size=10, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4908, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "    Args:\n",
    "        output_size(tuple or int): Desired output size. If tuple, output is matched to output_size.\n",
    "        If int, samller of image edges is matched to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self,output_size):\n",
    "        assert isinstance(output_size, (int,tuple))\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self,sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h>w:\n",
    "                new_h, new_w = self.output_size*h/w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w/h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "            \n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = transform.resize(image, (new_h,new_w))\n",
    "        \n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        landmarks = landmarks * [new_w/w, new_h/h]\n",
    "        \n",
    "        return {'image': img, 'landmarks':landmarks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose transforms\n",
    "scale = Rescale(256)\n",
    "crop = RandomCrop(128)\n",
    "composed = transforms.Compose([Rescale(256),\n",
    "                               RandomCrop(224)])\n",
    "\n",
    "# Apply each of the above transforms on sample\n",
    "fig = plt.figure()\n",
    "sample = face_dataset[65]\n",
    "for i, tsfrm in enumerate([scale, crop, composed]):\n",
    "    transformed_sample = tsfrm(sample)\n",
    "    \n",
    "    ax = plt.subplot(1,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title(type(tsfrm).__name__)\n",
    "    show_landmarks(**transformed_sample)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1 = torch.zeros(1,1,5,5)\n",
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1[0,0,2,:] = 1\n",
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image1[0,0,:,:].numpy(),interpolation='nearest',cmap=plt.cm.gray)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[[[ 0.0067, -0.1090,  0.0653],\n",
       "                        [-0.2699, -0.3227,  0.1913],\n",
       "                        [-0.2483, -0.1949, -0.0818]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0576,  0.0624,  0.0517],\n",
       "                        [ 0.2165, -0.2231, -0.0102],\n",
       "                        [-0.0422, -0.2407, -0.2644]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0684, -0.1069,  0.1384],\n",
       "                        [-0.2560,  0.0364, -0.1857],\n",
       "                        [-0.2789,  0.0852,  0.3200]]]])),\n",
       "             ('bias', tensor([ 0.3006,  0.2495, -0.2534]))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3)\n",
    "conv1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gx = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]])\n",
    "Gy = torch.tensor([[1,2,1],[0,0,0],[-1,2,-1]])\n",
    "\n",
    "conv1.state_dict()['weight'][0][0] = Gx\n",
    "conv1.state_dict()['weight'][1][0] = Gy\n",
    "conv1.state_dict()['weight'][2][0] = torch.ones(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([[[[ 1.,  0., -1.],\n",
       "                        [ 2.,  0., -2.],\n",
       "                        [ 1.,  0., -1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.,  2.,  1.],\n",
       "                        [ 0.,  0.,  0.],\n",
       "                        [-1.,  2., -1.]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.,  1.,  1.],\n",
       "                        [ 1.,  1.,  1.],\n",
       "                        [ 1.,  1.,  1.]]]])), ('bias', tensor([0., 0., 0.]))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.state_dict()['bias'][:] =torch.tensor([0,0,0])\n",
    "conv1.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [4., 4., 4.]],\n",
       "\n",
       "         [[3., 3., 3.],\n",
       "          [3., 3., 3.],\n",
       "          [3., 3., 3.]]]], grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = conv1(image1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [4., 4., 4.]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for channel, image in enumerate(out[0]):\n",
    "    plt.imshow(image.detach().numpy(), interpolation='nearest', cmap=plt.cm.gray)\n",
    "    print(image)\n",
    "    plt.title(\"channel {}\".format(channel))\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for plotting the channels\n",
    "\n",
    "def plot_channels(W):\n",
    "    n_out = W.shape[0]\n",
    "    n_in = W.shape[1]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(n_out, n_in)\n",
    "    fig.subplots_adjust(hspace=0.1)\n",
    "    out_index = 0\n",
    "    in_index = 0\n",
    "    \n",
    "    # plot outputs as rows inputs as columns\n",
    "    for ax in axes.flat:\n",
    "        if in_index > n_in-1:\n",
    "            out_index = out_index + 1\n",
    "            in_index = 0\n",
    "        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index = in_index + 1\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for plotting the parameters\n",
    "def plot_parameters(W, number_rows=1, name=\"\", i=0):\n",
    "    W = W.data[:,i,:,:]\n",
    "    n_filters = W.shape[0]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_filters//number_rows)\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "    \n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        if i<n_filters:\n",
    "            # set the label for the subplot\n",
    "            ax.set_xlabel(\"kernel:{0}\".format(i+1))\n",
    "            \n",
    "            # plot the image\n",
    "            ax.imshow(W[i,:],vmin=w_min,vmax=w_max,cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.suptitle(name,fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for plotting the activations\n",
    "def plot_activations(A, number_rows=1, name=\"\", i=0):\n",
    "    A = A[0,:,:,:].detach().numpy()\n",
    "    n_activations = A.shape[0]\n",
    "    A_min = A.min().item()\n",
    "    A_max = A.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "    \n",
    "    for i,ax in enumerate(axes.flat):\n",
    "        if i < n_activations:\n",
    "            # Set the label for the subplot \n",
    "            ax.set_xlabel(\"activation:{0}\".format(i+1))\n",
    "            \n",
    "            # plot the image\n",
    "            ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE,IMAGE_SIZE),cmap='gray')\n",
    "    plt.title('y= '+ str(data_sample[1].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
